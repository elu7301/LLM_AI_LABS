{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport torch\nfrom datasets import load_dataset\nfrom transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom peft import PromptTuningConfig, get_peft_model, TaskType, PromptTuningInit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:13:57.715897Z","iopub.execute_input":"2025-02-27T13:13:57.716207Z","iopub.status.idle":"2025-02-27T13:14:19.688422Z","shell.execute_reply.started":"2025-02-27T13:13:57.716177Z","shell.execute_reply":"2025-02-27T13:14:19.687737Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:14:19.689685Z","iopub.execute_input":"2025-02-27T13:14:19.690357Z","iopub.status.idle":"2025-02-27T13:14:19.740642Z","shell.execute_reply.started":"2025-02-27T13:14:19.690325Z","shell.execute_reply":"2025-02-27T13:14:19.739836Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"dataset = load_dataset(\"imdb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:14:19.742616Z","iopub.execute_input":"2025-02-27T13:14:19.742830Z","iopub.status.idle":"2025-02-27T13:14:30.601088Z","shell.execute_reply.started":"2025-02-27T13:14:19.742812Z","shell.execute_reply":"2025-02-27T13:14:30.600400Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f19b8eac14a468c92fec37bafba803d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89c90619b9964f7c87c75f689451819e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d91a8b4578143d6bf82a134e6704d22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b8736fbcf64d148ecbe7f0b7125f46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b41e7377bdb948b593a454f0afa14dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e92aabbe1d564c56b58e9966d5787806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c197902138e4f569276c1eb1411e57c"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:14:30.602469Z","iopub.execute_input":"2025-02-27T13:14:30.602742Z","iopub.status.idle":"2025-02-27T13:14:34.995311Z","shell.execute_reply.started":"2025-02-27T13:14:30.602720Z","shell.execute_reply":"2025-02-27T13:14:34.994365Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc5debc423b74b19b93acf0504483bd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e49cae002442e5b81ed95767070ef0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3ec54cc89e146f7a6525b06dfe423b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"962f1318c1ed4029909707341db0b039"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8df89c6657940a58eeca18ce4d5fdd5"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\ntrain_dataset = tokenized_datasets[\"train\"]\neval_dataset = tokenized_datasets[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:14:34.996217Z","iopub.execute_input":"2025-02-27T13:14:34.996480Z","iopub.status.idle":"2025-02-27T13:18:16.311542Z","shell.execute_reply.started":"2025-02-27T13:14:34.996458Z","shell.execute_reply":"2025-02-27T13:18:16.310813Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f6bae69f3d547a18243bcbe01c97990"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6327b47a1340407dbd0921a1b45b899e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f916e3da986a452a9e97b0e8acaae14d"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:18:16.312313Z","iopub.execute_input":"2025-02-27T13:18:16.312605Z","iopub.status.idle":"2025-02-27T13:18:16.316764Z","shell.execute_reply.started":"2025-02-27T13:18:16.312584Z","shell.execute_reply":"2025-02-27T13:18:16.315856Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"virtual_tokens_list = [5, 20, 50, 100]\n\nresults_dict = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:18:16.317673Z","iopub.execute_input":"2025-02-27T13:18:16.317980Z","iopub.status.idle":"2025-02-27T13:18:16.363258Z","shell.execute_reply.started":"2025-02-27T13:18:16.317943Z","shell.execute_reply":"2025-02-27T13:18:16.362545Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\nfor num_virtual_tokens in virtual_tokens_list:\n    print(f\"\\nЗапуск обучения с {num_virtual_tokens} виртуальными токенами\")\n    \n    base_model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n    base_model.config.pad_token_id = tokenizer.eos_token_id\n\n    peft_config = PromptTuningConfig(\n        task_type=TaskType.SEQ_CLS,\n        num_virtual_tokens=num_virtual_tokens,\n        prompt_tuning_init=PromptTuningInit.RANDOM,\n        tokenizer_name_or_path=\"gpt2\"\n    )\n    \n    model = get_peft_model(base_model, peft_config)\n    model.to(device)\n    \n    training_args = TrainingArguments(\n        output_dir=f\"./results/prompt_{num_virtual_tokens}\",\n        num_train_epochs=1,\n        per_device_train_batch_size=1,\n        per_device_eval_batch_size=1,\n        eval_strategy=\"epoch\",\n        logging_strategy=\"no\",\n        learning_rate=5e-5,\n        weight_decay=0.01,\n        report_to=\"none\",\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n    )\n    \n    if torch.cuda.is_available():\n        torch.cuda.reset_max_memory_allocated()\n    \n    start_time = time.time()\n    trainer.train()\n    elapsed_time = time.time() - start_time\n    \n    eval_results = trainer.evaluate()\n    \n    if torch.cuda.is_available():\n        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n    else:\n        max_memory = None\n\n    results_dict[num_virtual_tokens] = {\n        \"training_time_sec\": elapsed_time,\n        \"eval_results\": eval_results,\n        \"max_gpu_memory_MB\": max_memory,\n    }\n    \n    print(f\"Виртуальные токены: {num_virtual_tokens}\")\n    print(f\"Время обучения: {elapsed_time:.2f} секунд\")\n    print(\"Метрики на валидации:\", eval_results)\n    if max_memory:\n        print(f\"Максимальное использование GPU памяти: {max_memory:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T13:18:16.364047Z","iopub.execute_input":"2025-02-27T13:18:16.364323Z","iopub.status.idle":"2025-02-27T17:32:02.687116Z","shell.execute_reply.started":"2025-02-27T13:18:16.364296Z","shell.execute_reply":"2025-02-27T17:32:02.686076Z"}},"outputs":[{"name":"stdout","text":"\nЗапуск обучения с 5 виртуальными токенами\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd32fc14b8b4879997229a01ec25b8c"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-12-691bac62544a>:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\nGPT2ForSequenceClassification will not detect padding tokens in `inputs_embeds`. Results may be unexpected if using padding tokens in conjunction with `inputs_embeds.`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25000/25000 49:22, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.568673</td>\n      <td>0.509320</td>\n      <td>0.438492</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25000/25000 09:32]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Виртуальные токены: 5\nВремя обучения: 2963.51 секунд\nМетрики на валидации: {'eval_loss': 1.5686726570129395, 'eval_accuracy': 0.50932, 'eval_f1': 0.4384921746462307, 'eval_runtime': 572.7175, 'eval_samples_per_second': 43.652, 'eval_steps_per_second': 43.652, 'epoch': 1.0}\nМаксимальное использование GPU памяти: 962.88 MB\n\nЗапуск обучения с 20 виртуальными токенами\n","output_type":"stream"},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-12-691bac62544a>:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25000/25000 50:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.011253</td>\n      <td>0.511520</td>\n      <td>0.455886</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25000/25000 09:53]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Виртуальные токены: 20\nВремя обучения: 3038.28 секунд\nМетрики на валидации: {'eval_loss': 1.011252522468567, 'eval_accuracy': 0.51152, 'eval_f1': 0.45588630183053674, 'eval_runtime': 593.2737, 'eval_samples_per_second': 42.139, 'eval_steps_per_second': 42.139, 'epoch': 1.0}\nМаксимальное использование GPU памяти: 968.08 MB\n\nЗапуск обучения с 50 виртуальными токенами\n","output_type":"stream"},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-12-691bac62544a>:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25000/25000 51:30, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.091276</td>\n      <td>0.510280</td>\n      <td>0.396193</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25000/25000 09:58]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Виртуальные токены: 50\nВремя обучения: 3091.07 секунд\nМетрики на валидации: {'eval_loss': 1.0912758111953735, 'eval_accuracy': 0.51028, 'eval_f1': 0.39619257889815895, 'eval_runtime': 598.5305, 'eval_samples_per_second': 41.769, 'eval_steps_per_second': 41.769, 'epoch': 1.0}\nМаксимальное использование GPU памяти: 1003.24 MB\n\nЗапуск обучения с 100 виртуальными токенами\n","output_type":"stream"},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-12-691bac62544a>:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25000/25000 1:00:54, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.137965</td>\n      <td>0.510320</td>\n      <td>0.377204</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25000/25000 11:47]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Виртуальные токены: 100\nВремя обучения: 3655.14 секунд\nМетрики на валидации: {'eval_loss': 1.137965202331543, 'eval_accuracy': 0.51032, 'eval_f1': 0.37720364195111994, 'eval_runtime': 707.1165, 'eval_samples_per_second': 35.355, 'eval_steps_per_second': 35.355, 'epoch': 1.0}\nМаксимальное использование GPU памяти: 1044.43 MB\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"\\nСводка результатов по количеству виртуальных токенов:\")\nfor vt, metrics in results_dict.items():\n    print(f\"Виртуальные токены: {vt} -> Время: {metrics['training_time_sec']:.2f} сек, \"\n          f\"Метрики: {metrics['eval_results']}, \"\n          f\"GPU память: {metrics['max_gpu_memory_MB']:.2f} MB\" if metrics['max_gpu_memory_MB'] is not None else \"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T17:32:02.689797Z","iopub.execute_input":"2025-02-27T17:32:02.690069Z","iopub.status.idle":"2025-02-27T17:32:02.696438Z","shell.execute_reply.started":"2025-02-27T17:32:02.690045Z","shell.execute_reply":"2025-02-27T17:32:02.695630Z"}},"outputs":[{"name":"stdout","text":"\nСводка результатов по количеству виртуальных токенов:\nВиртуальные токены: 5 -> Время: 2963.51 сек, Метрики: {'eval_loss': 1.5686726570129395, 'eval_accuracy': 0.50932, 'eval_f1': 0.4384921746462307, 'eval_runtime': 572.7175, 'eval_samples_per_second': 43.652, 'eval_steps_per_second': 43.652, 'epoch': 1.0}, GPU память: 962.88 MB\nВиртуальные токены: 20 -> Время: 3038.28 сек, Метрики: {'eval_loss': 1.011252522468567, 'eval_accuracy': 0.51152, 'eval_f1': 0.45588630183053674, 'eval_runtime': 593.2737, 'eval_samples_per_second': 42.139, 'eval_steps_per_second': 42.139, 'epoch': 1.0}, GPU память: 968.08 MB\nВиртуальные токены: 50 -> Время: 3091.07 сек, Метрики: {'eval_loss': 1.0912758111953735, 'eval_accuracy': 0.51028, 'eval_f1': 0.39619257889815895, 'eval_runtime': 598.5305, 'eval_samples_per_second': 41.769, 'eval_steps_per_second': 41.769, 'epoch': 1.0}, GPU память: 1003.24 MB\nВиртуальные токены: 100 -> Время: 3655.14 сек, Метрики: {'eval_loss': 1.137965202331543, 'eval_accuracy': 0.51032, 'eval_f1': 0.37720364195111994, 'eval_runtime': 707.1165, 'eval_samples_per_second': 35.355, 'eval_steps_per_second': 35.355, 'epoch': 1.0}, GPU память: 1044.43 MB\n","output_type":"stream"}],"execution_count":13}]}